{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of dalle_back.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRRXnKUCCMTv"
      },
      "source": [
        "# **This colab allows users to inference and share released Dall-E Models.**\n",
        "\n",
        "# [Dall-E Service](https://github.com/rom1504/dalle-service) | [Models](https://github.com/robvanvolt/DALLE-models)\n",
        "\n",
        "*Colab created by mega b#6696*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrmBb36yn4i_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c9b8192-991c-4e07-d430-93cb672acc21"
      },
      "source": [
        "#@title # **Setup, run this once**\n",
        "from IPython.display import clear_output\n",
        "!sudo apt-get -y install llvm-9-dev cmake\n",
        "!git clone https://github.com/microsoft/DeepSpeed.git /tmp/Deepspeed\n",
        "%cd /tmp/Deepspeed\n",
        "!DS_BUILD_SPARSE_ATTN=1 ./install.sh -r\n",
        "!npm install -g localtunnel\n",
        "clear_output()\n",
        "%cd /content/\n",
        "!pip install Flask==1.1.2 Flask-Cors==3.0.9 Flask-RESTful==0.3.8 dalle-pytorch tqdm\n",
        "!git clone https://github.com/rom1504/dalle-service.git\n",
        "clear_output()\n",
        "print(\"Finished setup.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished setup.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B__cvu7glQZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f45f151-c73a-456a-a24b-df802c1a3e31"
      },
      "source": [
        "#@title # **Enter direct model download**\n",
        "#@markdown # Publicly released models are located [here](https://github.com/robvanvolt/DALLE-models).\n",
        "\n",
        "model_url = \"https://github.com/johnpaulbin/DALLE-models/releases/download/model/16L_64HD_8H_512I_128T_cc12m_cc3m_3E.pt\" #@param {type:\"string\"}\n",
        "\n",
        "!wget \"$model_url\" -O dalle_checkpoint.pt\n",
        "\n",
        "!echo '{\"good\": \"dalle_checkpoint.pt\"}' > pretrained_models.json\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(\"Finished download.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished download.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCAhqNj0o0J3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f746f0-39f0-4231-f05f-9da2a0eee14d"
      },
      "source": [
        "#@title # **Start backend**\n",
        "#@markdown ## Copy the url it provides you, and you will be able to use it in https://rom1504.github.io/dalle-service\n",
        "\n",
        "#@markdown #### Example: https://rom1504.github.io/dalle-service?backendUrl=https://XXXX.loca.lt\n",
        "\n",
        "from threading import Thread\n",
        "\n",
        "def app():\n",
        "  !python dalle-service/dalle_server/app.py 8000\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    t1 = Thread(target = app)\n",
        "    a = t1.start()\n",
        "    !lt --port 8000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "your url is: https://clever-pug-26.loca.lt\n",
            "100%|██████████████████████████████████████| 645/645 [00:00<00:00, 11234.28it/s]\n",
            "100%|███████████████████████| 957954257/957954257 [01:01<00:00, 15621858.41it/s]\n",
            "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100% 528M/528M [00:03<00:00, 176MB/s]\n",
            "Downloading vgg_lpips model from https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1 to taming/modules/autoencoder/lpips/vgg.pth\n",
            "8.19kB [00:00, 351kB/s]        \n",
            "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
            "VQLPIPSWithDiscriminator running with hinge loss.\n",
            "Loaded VQGAN from /root/.cache/dalle/vqgan.1024.model.ckpt and /root/.cache/dalle/vqgan.1024.config.yml\n",
            " * Serving Flask app \"app\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n",
            " * Running on http://0.0.0.0:8000/ (Press CTRL+C to quit)\n",
            "127.0.0.1 - - [18/Jul/2021 11:18:07] \"\u001b[37mOPTIONS /available-models HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [18/Jul/2021 11:18:07] \"\u001b[37mGET /available-models HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [18/Jul/2021 11:18:13] \"\u001b[37mOPTIONS /dalle HTTP/1.1\u001b[0m\" 200 -\n",
            "generating images for - tensor([[ 320, 2368, 1105,  539, 1704,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [ 320, 2368, 1105,  539, 1704,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [ 320, 2368, 1105,  539, 1704,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [ 320, 2368, 1105,  539, 1704,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0],\n",
            "        [ 320, 2368, 1105,  539, 1704,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "            0,    0,    0,    0,    0,    0,    0,    0]], device='cuda:0'):   0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}